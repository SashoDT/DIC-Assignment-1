# Difficulties: 
Very difficult to understand how syntax of MRJob works, 
e.g.    how does DIC_runner call "make_runner()", "cat_output()", "parse_output()"; 
        what is a runner, how does MRJob even run, does it always call a function named steps()? 
Also difficulties to understand how to pass files with bash into python script 
Difficult to figure out how to compute A,B,C,D variables 

# Steps: 
First added the prep.py lines to our mapper; oriented by using assignment0 files 

Noticed that reading in stopwords file for every json-file line is inefficient 
-> Lead to v2, where stopwords file location can be passed in bash and read in only once 
also yields key pairs instead 

Next implement mapper and reducer to yield variables for ChiSquare computation: 
- Added 4 yield commands to mapper, to count different variables, 
every variable in ChiSquare can be computed from those numbers (big breakthrough). 
- Reducer is just the counter from assignment0 
- Next have to edit the runner script to give out the counters and compute 
ChiSquare statistic 

Now we need to order the terms and preserve the top 75 terms per category: 
- Sort terms by ChiSquare values and keep top 75 by using nlargest()
- Output just by using simple for-loop and print() 


# Run: 
Use sth like this to run in console (change "local" if ran in lbd and add paths): 
"python DIC_runner.py -r local --stopwords stopwords.txt reviews_devset.json > word_count.txt"

How to run on lbd cluster: 
- create zip-file out of "DIC_runner", "MapReduce_v2", "stopwords.txt" and maybe small json data file into folder Assignment1 (name here: Assignment1.zip)
- upload: scp Assignment1.zip e{studentID}@lbd.tuwien.ac.at:. 
- login: ssh e{studentID}@lbd.tuwien.ac.at 
- unzip: unzip Assignment1.zip
- change directory if zip contains folder Assignment1, else skip: cd Assignment1
- list files, if needed: ls - 1
- run (small dataset): 
    python DIC_runner.py --stopwords stopwords.txt --hadoop-streaming-jar \
    /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \
    -r hadoop hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json > output.txt

- run (large dataset): 
    python DIC_runner.py --stopwords stopwords.txt --hadoop-streaming-jar \
    /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \
    -r hadoop hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json > output.txt

**Recap of the variable names for ChiSquare from the lecture:** 
- t ... term  
- c ... category  
- A ... number of documents in c which contain t  
- B ... number of documents not in c which contain t  
- C ... number of documents in c without t  
- D ... number of documents not in c without t  
- N ... total number of retrieved documents  (can be ommited for ranking)


# New Chapter: MapReduce_v3 
Need to improve computation time drastically by introducing: 
- Major change: Combiner, what it does: https://www.geeksforgeeks.org/mapreduce-combiners/ 
- Minor change: Moved "delimiters" to stopwords mapper to reduce calling and computing this often (per json record)
- Minor change: Skipped try/except with get(filename, "") and set.discard("")

Also added file DIC_TestRunner_only_testing.py which is only for testing. 
It measures the time and outputs it at the end of the output.txt file. 
Note that if it runs like 2 min on the small dataset, this would not mean that 
it would need 2*1000 min to run on the large dataset. The combiner works best on large datasets. 
Also note that the time also depends on the server load. 

# Results log of small dataset: 

- 24.04.2025, original MapReduce_v2, DIC_runner: 
    start: 1745493540.5206335
    MRJob: 1745493654.9700916
    end: 1745493655.7207308
    MRJob Time elapsed: 114.44945812225342
    Total Time elapsed: 115.20009732246399

- 24.04.2025, MapReduce_v3 (add combiner), DIC_runner: 
    start: 1745494861.717979
    MRJob: 1745494968.9713297
    end: 1745494969.5808668
    MRJob Time elapsed: 107.2533507347107
    Total Time elapsed: 107.86288785934448

- 24.04.2025, MapReduce_v3 (add combiner, move delimiters), DIC_runner: 
    start: 1745497647.6800425
    MRJob: 1745497750.851694
    end: 1745497751.6463783
    MRJob Time elapsed: 103.17165160179138
    Total Time elapsed: 103.96633577346802

    start: 1745498324.1590958
    MRJob: 1745498424.28417
    end: 1745498425.0451283
    MRJob Time elapsed: 100.1250741481781
    Total Time elapsed: 100.88603258132935

# Results log of large dataset: 
- 24.04.2025, MapReduce_v3 (add combiner, move delimiters), DIC_runner: 
    start: 1745498513.4807856
    MRJob: 1745500334.4827776
    end: 1745500365.1184752
    MRJob Time elapsed: 1821.0019919872284
    Total Time elapsed: 1851.637689590454

- 24.04.2025, MapReduce_v3 (add combiner, move delimiters), DIC_runner_v2: 
    start: 1745501039.3094435
    MRJob: 1745503167.0330524
    end: 1745503198.280696
    MRJob Time elapsed: 2127.723608970642
    Total Time elapsed: 2158.9712524414062